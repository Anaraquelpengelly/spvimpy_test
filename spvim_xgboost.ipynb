{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from altair.vegalite.v4.schema.core import InlineData\n",
    "from numpy.core.numeric import outer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"http://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = data.iloc[0:data.shape[0] , 1:data.shape[1]]\n",
    "#%%\n",
    "heart['famhist'] = np.where(heart['famhist'] == 'Present',1, 0 )\n",
    "#%%\n",
    "X= heart.drop(columns='chd')\n",
    "X=X.to_numpy()\n",
    "y=np.array(heart.chd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use the cross validation function of xgb. we give the function an algorithm ( the xgb classifier), the train dataframe, the predictors\n",
    "# TODO: need to integrate teh test set in there too!! (inside the function fo the split)  \n",
    "def modelfit(alg, dtrain, y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        print('using train cv')\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain, label=y)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain, y, eval_metric='auc')\n",
    "    print('data fitted')    \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain)\n",
    "    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n",
    "    #TODO: not sure what the above does, what is the pred_prob?\n",
    "\n",
    "    #Print model report:\n",
    "    print (f\"\\nModel Report\\nAccuracy : {accuracy_score(y, dtrain_predictions)}\\nAUC Score (Train): {roc_auc_score(y, dtrain_predprob)}\")\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb0=XGBClassifier(learning_rate = 0.1,\n",
    "n_estimators=1000,\n",
    "max_depth=5, #max depth of a tree\n",
    "min_child_weight=1, # weight of leaf? needs to be tuned too high values can lead to underfitting.\n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "col_sample_bytree=0.8, #prop of obs to be used for each tree. \n",
    "objective='binary:logistic',\n",
    "nthread=4, #used for parallel processing! enter number of cores in the system.\n",
    "scale_pos_weight=1, # for high class imbalance!helps in fater convergence..\n",
    "seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets not use the function \n",
    "# modelfit(xgb0, X, y)\n",
    "#we then first get the data matrix:\n",
    "data_matrix=xgb.DMatrix(data=X, label=y)\n",
    "#do the train/test split:\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, random_state=123) \n",
    "xgb0.fit(X_train,y_train, eval_metric='auc')\n",
    "preds=xgb0.predict(X_test)\n",
    "print (f\"\\nModel Report\\nAccuracy : {accuracy_score(y, preds)}\\nAUC Score (Train): {roc_auc_score(y, preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spvim_env",
   "language": "python",
   "name": "spvim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
